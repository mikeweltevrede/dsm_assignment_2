---
title: "Data Science Methods - Homework Assignment 2"
author: "Group 20: Steffie van Poppel (2031218), Robbie Reyerse (2039047), Mike Weltevrede (1257560)"
date: "March 21, 2020"
header-includes:
   - \usepackage{booktabs}
   - \usepackage{arydshln}
   - \usepackage{float}
   - \usepackage{caption}
   - \usepackage{graphicx}
   - \usepackage{titling}
   - \pretitle{\begin{center}\LARGE\includegraphics[width=12cm]{images/TiuLogo.eps}\\[\bigskipamount]}
   - \posttitle{\end{center}}
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

library(glmnet)
library(pROC)
library(caret)
library(knitr)
```

# Exercise 1
In this exercise, we are asked to apply two methods (we choose to apply a LASSO and a ridge regression) to a dataset on many macroeconomic variables to forecast financial crises. The dataset that we will use was also used by Ward (2017, Journal of Applied Econometrics) where they contrast the performance of one tree with bagging and a random forest against the logit benchmark. We wish to compare our results to those as presented in Table \ref{tab:resultsward}.

\begin{table}[H]
\centering
\begin{tabular}{lccccccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{7}{c}{\textbf{Results}} \\
\cmidrule(l r){2-8} \\
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Restricted Selection}} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Many Predictors}} \\
\cmidrule(l r){2-4} \cmidrule(l r){6-8} \\
\textbf{Model} & AUC & 95\%-CI & N &  & AUC & 95\%-CI & N \\
\cdashline{1-8} \\
Single Tree & 0.55 & [0.49,0.6] & 1816 &  & 0.63 & [0.56,0.69] & 1742 \\
Bagging & \textbf{ 0.77 } & [0.73,0.81] & 1816 &  & \textbf{ 0.87 } & [0.84,0.9] & 1742 \\
Random Forest & \textbf{ 0.79 } & [0.75,0.83] & 1816 &  & \textbf{ 0.88 }  & [0.86,0.91] & 1742 \\
\\
\multicolumn{1}{c}{} & \multicolumn{7}{c}{\textbf{Specification}} \\
\cmidrule(l r){2-8} \\
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Restricted Selection}} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Many Predictors}} \\
\cmidrule(l r){2-4}  \cmidrule(l r){6-8} \\
\textbf{Parameter} & Single & Bagging & RF &  & Single & Bagging & RF \\
\cdashline{1-8} \\
B & 1 & 5000 & 5000 &  & 1 & 5000 & 5000 \\
$ J_{try} $ & 10 & 10 & 3 &  & 76 & 76 & 9 \\
$ J $ &  & 10 &  &  &  & 76 &  \\
\# of crises &  & 72 &  &  &  & 70 &  \\
\bottomrule
\end{tabular}
\caption{Results from Ward (2017).}
\label{tab:resultsward}
\end{table}

We first need to do data preparation. We follow the same data preparation procedure as Ward. We are, however, only interested in the case where ``many predictors" are used.

```{r Data Preparation}
data_path = "data"
df_data  = read.table(paste0(data_path, "/R_class.csv"), sep=",", dec=".",
                      header=TRUE)

ca = grep("ca", names(df_data), value=T)
df_data = df_data[!(names(df_data) %in% c(ca))]

# drop vars not used
stocks = grep("stocks", names(df_data), value=T)
money = grep("money", names(df_data), value=T)
stir = grep("stir", names(df_data), value=T)
assets = grep("assets", names(df_data), value=T)
i = grep("i_", names(df_data), value=T)
ri = grep("ri", names(df_data), value=T)
glo = grep("a_", names(df_data), value=T)

drops = names(df_data) %in% c("year", "ccode", stocks, money, stir, assets, i,
                              ri, glo)
full_om = na.omit(cbind(df_data[glo], df_data[!drops]))
```

Next, we run a LASSO and a ridge regression model. The function that we use for this is `lasso_ridge_sim`. Using the parameter `alpha`, we can specify whether we want a LASSO regression (`alpha=1`) or a Ridge regression (`alpha=0`).

```{r Models}
lasso_ridge_sim = function(data, grid_lambda = 10^seq(2, -3, length=100),
                           alpha=1, num_runs=100){

  lambdas = vector("numeric", num_runs)
  
  if (alpha==1){
    nzeros = vector("numeric", num_runs)
  }
  
  auc = vector("numeric", num_runs)
  ci95_auc_lo = vector("numeric", num_runs)
  ci95_auc_up = vector("numeric", num_runs)
  precisions = vector("numeric", num_runs)
  recalls = vector("numeric", num_runs)
  f_measures = vector("numeric", num_runs)
  
  for(j in 1:num_runs) {
    
    set.seed(j)
    
    # Select training and test data
    train_labels = sample(1:nrow(data), floor(nrow(data)*0.5))
    train = data[train_labels, ]
    test = data[-train_labels, ]
    train_matrix = model.matrix(b2 ~ ., data=train)
    test_matrix = model.matrix(b2 ~ ., data=test)
    
    # Train the LASSO/Ridge model
    model = glmnet::cv.glmnet(train_matrix, train[, "b2"], alpha=alpha,
                              lambda=grid_lambda, thresh=1e-12,
                              family="binomial")
    
    lambdas[j] = model$lambda.1se
    
    if (alpha==1){
      nzeros[j] = model$nzero[[which(model$lambda == model$lambda.1se)]]
    }
    
    # Test the LASSO/Ridge model
    prediction = predict(model, newx=test_matrix, s=model$lambda.1se,
                         type ="class")
    
    # ROC analysis
    r = pROC::roc(test[, "b2"], as.numeric(prediction), ci=T, quiet=T)
    auc[j] = as.numeric(r$auc)
    
    ci95_auc_lo[j] = as.numeric(ci.auc(r, conf.level = r$ci[2]))[1]
    ci95_auc_up[j] = as.numeric(ci.auc(r, conf.level = r$ci[2]))[3]
    
    # Classification evaluation methods
    precisions[j] = caret::precision(factor(prediction, levels=c(0,1)),
                                     factor(test$b2, levels=c(0,1)))
    recalls[j] = caret::recall(factor(prediction, levels=c(0,1)),
                               factor(test$b2, levels=c(0,1)))
    f_measures[j] = caret::F_meas(factor(prediction, levels=c(0,1)),
                                  factor(test$b2, levels=c(0,1)))
  }
  
  results = list(auc = mean(auc),
                 ci95_auc_lo = mean(ci95_auc_lo),
                 ci95_auc_up = mean(ci95_auc_up),
                 precision = mean(precisions),
                 ci95_precision_lo = mean(precisions) - qnorm(0.975)*
                   sd(precisions) / sqrt(nrow(test_matrix)),
                 ci95_precision_up = mean(precisions) + qnorm(0.975)*
                   sd(precisions) / sqrt(nrow(test_matrix)),
                 recall = mean(recalls),
                 ci95_recall_lo = mean(recalls) - qnorm(0.975)*sd(recalls) / 
                   sqrt(nrow(test_matrix)),
                 ci95_recall_up = mean(recalls) + qnorm(0.975)*sd(recalls) /
                   sqrt(nrow(test_matrix)),
                 f_measure = mean(f_measures),
                 ci95_f_measure_lo = mean(f_measures) - qnorm(0.975)*
                   sd(f_measures) / sqrt(nrow(test_matrix)),
                 ci95_f_measure_up = mean(f_measures) + qnorm(0.975)*
                   sd(f_measures) / sqrt(nrow(test_matrix)),
                 lambda = mean(lambdas),
                 ci95_lambdas_lo = mean(lambdas) - qnorm(0.975)*sd(lambdas) /
                   sqrt(nrow(test_matrix)),
                 ci95_lambdas_up = mean(lambdas) + qnorm(0.975)*sd(lambdas) /
                   sqrt(nrow(test_matrix))
                 )
  
  if (alpha == 1){
    results[["nzeros"]] = mean(nzeros)
    results[["ci95_nzeros_lo"]] = mean(nzeros) - qnorm(0.975)*sd(nzeros) /
      sqrt(nrow(test_matrix))
    results[["ci95_nzeros_up"]] = mean(nzeros) + qnorm(0.975)*sd(nzeros) /
      sqrt(nrow(test_matrix))
  }
  
  return(results)
}

lasso_results = lasso_ridge_sim(full_om)
ridge_results = lasso_ridge_sim(full_om, alpha=0)
```

In addition to the AUC used by Ward, we have also added the precision, recall, and F-measure (AKA the F-Score or the F1-Score) to our analysis. These are commonly used evaluation metrics for categorical response variables. Since our variable is binary, these are applicable. We did not use accuracy as a measure since we have only `r table(full_om$b2)[["1"]]` observations where our variable has value 1 compared to `r table(full_om$b2)[["0"]]` observations with value 0. As such, the majority class of zeros will overpower the minority class with value 1.

For completeness sake, these are the definitions of the three metrics:

- $Precision = \frac{\textit{\#True Positives}}{\textit{\#True Positives + \#False Positives}}$, i.e. if we predict a crisis, how often is this true?
- $Recall = \frac{\textit{\#True Positives}}{\textit{\#True Positives + \#False Negatives}}$, i.e. out of the total number of crises, how many do we correctly identify?
- $\textit{F-measure} = \frac{2 \times Precision \times Recall}{Precision + Recall}$, i.e. the harmonic mean of the precision and recall scores. This balances the precision and recall measures. It is often used to provide an additional check to values of precision and recall.

Given these definitions, we want a high precision if we want to minimise the number of false positives and a high recall when we want to minimise false negatives. In this case, our intuition would be that we would be most interested in the former. That is, we want to minimise the amount of times that a crisis is coming but that we do not identify this rather than minimising the amount of times where we say that a crisis is coming while this is not true. This is because a crisis can have a grave impact on many people in society and we would rather take too many precautions than too few. However, the other case is also not desirable; therefore, the $F$-measure is also important to consider.

The results from our analysis using the LASSO and ridge regression models can be found in Table \ref{tab:ourresults}.

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
                      & \multicolumn{4}{c}{\textbf{Results}}                                                                                    \\
\cmidrule(l r){2-5}                                                                                                                             \\
\textbf{Model}        & \multicolumn{1}{c}{AUC} & \multicolumn{1}{c}{Precision} & \multicolumn{1}{c}{Recall}    & \multicolumn{1}{c}{F-measure} \\
\cdashline{1-5}                                                                                                                                 \\
LASSO                 & `r round(lasso_results$auc, 4)` & `r round(lasso_results$precision, 4)` & `r round(lasso_results$recall, 4)` & `r round(lasso_results$f_measure, 4)` \\
                      & [`r round(lasso_results$ci95_auc_lo, 4)`, `r round(lasso_results$ci95_auc_up, 4)`] & [`r round(lasso_results$ci95_precision_lo, 4)`, `r round(lasso_results$ci95_precision_up, 4)`] & [`r round(lasso_results$ci95_recall_lo, 4)`, `r round(lasso_results$ci95_recall_up, 4)`] & [`r round(lasso_results$ci95_f_measure_lo, 4)`, `r round(lasso_results$ci95_f_measure_up, 4)`] \\
Ridge                 & `r round(ridge_results$auc, 4)` & `r round(ridge_results$precision, 4)` & `r round(ridge_results$recall, 4)` & `r round(ridge_results$f_measure, 4)` \\
                      & [`r round(ridge_results$ci95_auc_lo, 4)`, `r round(ridge_results$ci95_auc_up, 4)`] & [`r round(ridge_results$ci95_precision_lo, 4)`, `r round(ridge_results$ci95_precision_up, 4)`] & [`r round(ridge_results$ci95_recall_lo, 4)`, `r round(ridge_results$ci95_recall_up, 4)`] & [`r round(ridge_results$ci95_f_measure_lo, 4)`, `r round(ridge_results$ci95_f_measure_up, 4)`] \\ \\
                      & \multicolumn{4}{c}{\textbf{Specification}}                                                                              \\
\cmidrule(l r){2-5}                                                                                                                             \\
\textbf{Parameter}    & \multicolumn{2}{c}{LASSO}                           & \multicolumn{2}{c}{Ridge}                                         \\
\cdashline{1-5}                                                                                                                                 \\
$\lambda$             & \multicolumn{2}{c}{`r round(lasso_results$lambda, 4)`} & \multicolumn{2}{c}{`r round(ridge_results$lambda, 4)`}         \\
                      & \multicolumn{2}{c}{[`r round(lasso_results$ci95_lambdas_lo, 4)`, `r round(lasso_results$ci95_lambdas_up, 4)`]} & \multicolumn{2}{c}{[`r round(ridge_results$ci95_lambdas_lo, 4)`, `r round(ridge_results$ci95_lambdas_up, 4)`]} \\
Mean \# nonzero parameters & \multicolumn{2}{c}{`r round(lasso_results$nzeros, 4)`} & \multicolumn{2}{c}{-}                                             \\
                      & \multicolumn{2}{c}{[`r round(lasso_results$ci95_nzeros_lo, 4)`, `r round(lasso_results$ci95_nzeros_up, 4)`]} & \multicolumn{2}{c}{-}                                             \\
\bottomrule
\end{tabular}
\caption{Our results (Ranges indicate 95\% confidence intervals)}
\label{tab:ourresults}
\end{table}

Firstly, we will compare our results to that of Ward using the AUC for the ROC curve (hereafter referred to only as the AUC). After this, we will look into the measures of precision, recall, and the $F$-measure. We weigh these off to the AUC measure that Ward uses. Lastly, we look into whether these models actually are applicable to the specific data that we are analysing.

To first compare our results with those of Ward, we can only look at the AUC as a quantitative measure since Ward reports no other measures. Ward achieves AUC values of 0.63, 0.87, and 0.88 for the Single Tree, Bagging, and Random Forest methods, respectively. Our analyses using LASSO and ridge regression achieve AUC values of `r round(lasso_results$auc, 4)` and `r round(ridge_results$auc, 4)`. Because we want to achieve an AUC value as close to 1 as possible, we can see that the LASSO and ridge regressions perform a bit worse than the Bagging and Random Forest methods used by Ward. However, note that AUC is deemed to not be a good metric in case of imbalance in the response as we have explained before. This is because one can achieve a high AUC when the model can identify the majority class well even though it may be very bad at identifying the minority class. Therefore, we think that it would be better to consider precision, recall, and the $F$-measure.

In that case, we see that the LASSO model achieves the values `r round(lasso_results$precision, 4)`, `r round(lasso_results$recall, 4)`, and `r round(lasso_results$f_measure, 4)` for the precision, recall, and $F$-measure, respectively. The ridge model achieves the values `r round(ridge_results$precision, 4)`, `r round(ridge_results$recall, 4)`, and `r round(ridge_results$f_measure, 4)` for the precision, recall, and $F$-measure, respectively. One can see that the LASSO model has a higher precision while the ridge model achieves a higher recall and $F$-measure. Following our arguments of before, we should value recall a bit more than precision, though this is not set in stone. Therefore, we can also consider the $F$-measure as a balance between the two measures. In that case, the ridge model achieves a slightly higher value for the $F$-measure though not by much. Since the confidence interval of the $F$-measure for the ridge regression does not include the mean $F$-measure for the LASSO regression, it is significantly higher for the ridge regression.

We would then say that the choice comes to the final part of our analysis: do these models actually fit the data type? It is important to recognise that the LASSO model sets certain parameters to zero and that the ridge model shrinks them to zero. Although we cannot say that the LASSO model sets all parameters to zero that are truly zero (in exercise 2 we discuss that another model called adaptive LASSO is able to do this for estimation putposes), it seems intuitive that `r ncol(full_om)` variables are likely too many to describe whether a crisis would occur. The problem comes when we look at how many nonzero parameters LASSO selects, namely only 3.55 (on average). It seems unintuitive that only 3 to 4 parameters can accurately predict whether a crisis will occur. Nonetheless, apparently the LASSO model still does well on the basis of our evaluation metrics so it is not that bad. For the ridge model, some parameters are shrunk towards zero but none are set equal to zero (with probability 1). As such, all `r ncol(full_om)` variables are kept in the model but some will have lower coefficients. This makes the model in itself less intuitive to interpret but we are more interested in prediction than estimation so this is not a problem.

All in all, this means that we prefer the ridge model over the LASSO model. Comparing to the models by Ward, we can only use the AUC and our intuition on the applicability of the model to the specific data. On the AUC side, we saw that the AUC values for Ward's models were a bit higher than the LASSO and ridge models. However, also recall that this is not that applicable since the data is imbalanced in the response variable. Therefore, our final conclusion comes to the applicability of the models. Here, we unfortunately do not see a clear reason why we choose one model over the other purely on a theoretical level. As such, we make our conclusion based on the fact that the precision, recall, and $F$-measure values for the ridge regression model are quite high (also considering the lower bound of its confidence interval); high enough to choose this model over Ward's Random Forest, for example, as we do not have any information on this method's precision, recall, and $F$-measure.


## Exercise 2
### a. Plain versus adaptive LASSO
The objective functions of plain LASSO is shown in equation \eqref{eq:plain lasso}. 

\begin{equation}
    \min_\beta \Big( RSS + \lambda \sum_{j=1}^p |\beta_j| \Big),
    \label{eq:plain lasso}
\end{equation}

where the first part, the residuals sum of squares is defined in the usual way:
$$
RSS=\Big(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}\Big)^2
$$
The second part is the LASSO penalty, where $\lambda$ is the tuning parameter which controls the strength of the penalty.

Plain LASSO can be used when we assume $y=X\beta+\epsilon$, where $\epsilon$ is i.i.d and $\beta$ is sparse. The last means that a lot of coefficients should be believed to be zero. This is a reasonable thing to assume if the number of predictors, $p$, grows quickly with $n$. It will then set some of the true zero parameters to zero asymptotically (as $n\xrightarrow{}\infty$). For prediction it is not a problem that not all zero coefficients are set to zero. Since it safeguards that some coefficients will matter for out of sample while maybe in sample they did not in case of finite samples.

In case of prediction $\lambda$ is determined by choosing the lambda is that yields one standard deviation above the minimum cross-validation $\lambda$. If $\lambda=0$ there is no penalization and the plain LASSO solution will be identical to the least squares solution. On the other hand, when $\lambda=\infty$ all penalized coefficients will be zero.

Adaptive LASSO, however, is able to give the all the true zero coefficients. Which is essential for estimation (in sample). The objective of adaptive LASSO is shown in equation \eqref{eq:ad lasso}.

\begin{equation}
    \min_\beta\Big(RSS+\lambda \sum_{j=1}^p |\beta_j| w_j \Big)
        \label{eq:ad lasso}
\end{equation}

where,

\begin{equation}
    w_j = \frac{1}{|\hat{\beta}_j|^\gamma}~~\text{for}~ \gamma \geq 1
\label{eq:weight}
\end{equation}

In fact, when the same assumptions hold as for plain LASSO and in addition $\frac{\lambda_T}{\sqrt{T}}\xrightarrow{}0$ and $\lambda_T^{\frac{\gamma-1}{2}}\xrightarrow{} \infty$ adaptive LASSO selects the true non-zero coefficients with a probability 1 as $n\xrightarrow{}\infty$.

$\hat{\beta}_j\xrightarrow[]{P}\beta_j$ holds for $\hat{\beta}_j$ used in the weight of equation \eqref{eq:weight}, in other words the coefficients pre-estimates converge in probability to the true coefficients.

As can be seen in the objective function of adaptive LASSO, the only difference between the plain LASSO is the penalty. Here, it is weighted with $w_j$ (equation \eqref{eq:weight}). This means that penalization is done proportional to the values of the $\hat{\beta}_j$. So if the pre-estimates are large we penalize less and vise versa. The pre-estimates can be determined by e.g. plain LASSO.

In case we are interested in in-sample estimation, $\lambda$ can be chosen by BIC. However, there are still some problems because $\lambda$ is random. In the objective function, $\lambda$ is treated as only changing with the sample size, so it is not random. But then the $\beta$s are nonlinear functions of the random data. There has not yet been a good solution to this.

### Post-adaptive LASSO
When we are interested in estimation, we are usually interested in the effects of covariates on the dependent variable. Hence there is no need to report zero coefficients unless we want to report parameters that do not have a significant impact on the dependent variable. A main advantage of this is that, when we want to interpret a specific coefficient, we do not have to control for all the covariates for which its coefficient was proven to be zero by adaptive LASSO. When we would directly report the adaptive LASSO results, even though the coefficients were zero, there we had included them in the regression and therefore should be controlled for.

Moreover, although adaptive LASSO will consistently pick all the nonzero coefficients, it is biased in finite samples. For estimation this forms major problems. Post-LASSO estimates have been proven, by Windmeijer et al.\footnote{Windmeijer, F., Farbmacher, H., Davies, N., \& Davey Smith, G. (2019). On the use of the lasso for instrumental variables estimation with some invalid instruments. Journal of the American Statistical Association, 114(527), 1339-1350.}, to be less biased than LASSO estimates reported on its own.

In addition, Belloni et al.\footnote{Belloni, A., Chernozhukov, V., \& Hansen, C. (2014). Inference on treatment effects after selection among high-dimensional controls. The Review of Economic Studies, 81(2), 608-650.} showed that post-adaptive LASSO can improve inference on the parameter of interest because redundant regressors were penalized away in their procedure.

All in all, for estimation purpose - which are the case when one chooses adaptive LASSO instead of plain LASSO - post-adaptive LASSO will improve interpretation of the coefficients by decreasing the bias and it makes interpretation a lot easier since it is no longer needed to control for the zero coefficients predictors.

### b. Post-adaptive LASSO

## Exercise 3
### Subquestion a


### Subquestion b


### Subquestion c


### Subquestion d


### Subquestion e

