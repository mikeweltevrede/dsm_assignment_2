---
title: "Data Science Methods - Homework Assignment 2"
author: "Group 20: Steffie van Poppel (2031218), Robbie Reyerse (2039047), Mike Weltevrede (1257560)"
date: "March 21, 2020"
header-includes:
   - \usepackage{booktabs}
   - \usepackage{arydshln}
   - \usepackage{float}
   - \usepackage{caption}
   - \usepackage{graphicx}
   - \usepackage{titling}
   - \pretitle{\begin{center}\LARGE\includegraphics[width=12cm]{images/TiuLogo.eps}\\[\bigskipamount]}
   - \posttitle{\end{center}}
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())

library(glmnet)
library(pROC)
library(caret)
library(knitr)
```

# Assignment 1
In this exercise, we are asked to apply two methods to a dataset on many macroeconomic variables to forecast financial crises. The dataset that we will use was also used by Ward (2017, Journal of Applied Econometrics) where they contrast the performance of one tree with bagging and a random forest against the logit benchmark. We wish to compare our results to those as presented in \ref{tab:resultsward}.

\begin{table}[H]
\centering
\begin{tabular}{lccccccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{7}{c}{\textbf{Results}} \\
\cmidrule(l r){2-8} \\
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Restricted Selection}} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Many Predictors}} \\
\cmidrule(l r){2-4} \cmidrule(l r){6-8} \\
\textbf{Model} & AUC & 95\%-CI & N &  & AUC & 95\%-CI & N \\
\cdashline{1-8} \\
Single Tree & 0.55 & [0.49,0.6] & 1816 &  & 0.63 & [0.56,0.69] & 1742 \\
Bagging & \textbf{ 0.77 } & [0.73,0.81] & 1816 &  & \textbf{ 0.87 } & [0.84,0.9] & 1742 \\
Random Forest & \textbf{ 0.79 } & [0.75,0.83] & 1816 &  & \textbf{ 0.88 }  & [0.86,0.91] & 1742 \\
\\
\multicolumn{1}{c}{} & \multicolumn{7}{c}{\textbf{Specification}} \\
\cmidrule(l r){2-8} \\
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Restricted Selection}} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{Many Predictors}} \\
\cmidrule(l r){2-4}  \cmidrule(l r){6-8} \\
\textbf{Parameter} & Single & Bagging & RF &  & Single & Bagging & RF \\
\cdashline{1-8} \\
B & 1 & 5000 & 5000 &  & 1 & 5000 & 5000 \\
$ J_{try} $ & 10 & 10 & 3 &  & 76 & 76 & 9 \\
$ J $ &  & 10 &  &  &  & 76 &  \\
\# of crises &  & 72 &  &  &  & 70 &  \\
\bottomrule
\end{tabular}
\caption{Results from Ward (2017).}
\label{tab:resultsward}
\end{table}

We first need to do data preparation. We follow the same data preparation procedure as Ward. We are, however, only interested in the case where ``many predictors" are used.

```{r}
data_path = "data"
df_data  = read.table(paste0(data_path, "/R_class.csv"), sep=",", dec=".",
                      header=TRUE)

ca = grep("ca", names(df_data), value=T)
df_data = df_data[!(names(df_data) %in% c(ca))]

# drop vars not used
stocks = grep("stocks", names(df_data), value=T)
money = grep("money", names(df_data), value=T)
stir = grep("stir", names(df_data), value=T)
assets = grep("assets", names(df_data), value=T)
i = grep("i_", names(df_data), value=T)
ri = grep("ri", names(df_data), value=T)
glo = grep("a_", names(df_data), value=T)

drops = names(df_data) %in% c("year", "ccode", stocks, money, stir, assets, i,
                              ri, glo)
full_om = na.omit(cbind(df_data[glo], df_data[!drops]))
```

Next, we run a LASSO and a Ridge regression model. The function that we use for this is `lasso_ridge_sim`. Using the parameter `alpha`, we can specify whether we want a LASSO regression (`alpha=1`) or a Ridge regression (`alpha=0`).

```{r}
lasso_ridge_sim = function(data, grid_lambda = 10^seq(2, -3, length=100),
                           alpha=1, num_runs=100){

  lambdas = vector("numeric", num_runs)
  
  if (alpha==1){
    nzeros = vector("numeric", num_runs)
  }
  
  auc = vector("numeric", num_runs)
  ci95_auc_lo = vector("numeric", num_runs)
  ci95_auc_up = vector("numeric", num_runs)
  precisions = vector("numeric", num_runs)
  recalls = vector("numeric", num_runs)
  f_measures = vector("numeric", num_runs)
  
  for(j in 1:num_runs) {
    
    set.seed(j)
    
    # Select training and test data
    train_labels = sample(1:nrow(data), floor(nrow(data)*0.5))
    train = data[train_labels, ]
    test = data[-train_labels, ]
    train_matrix = model.matrix(b2 ~ ., data=train)
    test_matrix = model.matrix(b2 ~ ., data=test)
    
    # Train the LASSO/Ridge model
    model = glmnet::cv.glmnet(train_matrix, train[, "b2"], alpha=alpha,
                              lambda=grid_lambda, thresh=1e-12,
                              family="binomial")
    
    lambdas[j] = model$lambda.1se
    
    if (alpha==1){
      nzeros[j] = model$nzero[[which(model$lambda == model$lambda.1se)]]
    }
    
    # Test the LASSO/Ridge model
    prediction = predict(model, newx=test_matrix, s=model$lambda.1se,
                         type ="class")
    
    # ROC analysis
    r = pROC::roc(test[, "b2"], as.numeric(prediction), ci=T, quiet=T)
    auc[j] = as.numeric(r$auc)
    
    ci95_auc_lo[j] = as.numeric(ci.auc(r, conf.level = r$ci[2]))[1]
    ci95_auc_up[j] = as.numeric(ci.auc(r, conf.level = r$ci[2]))[3]
    
    # Classification evaluation methods
    precisions[j] = caret::precision(factor(prediction, levels=c(0,1)),
                                     factor(test$b2, levels=c(0,1)))
    recalls[j] = caret::recall(factor(prediction, levels=c(0,1)),
                               factor(test$b2, levels=c(0,1)))
    f_measures[j] = caret::F_meas(factor(prediction, levels=c(0,1)),
                                  factor(test$b2, levels=c(0,1)))
  }
  
  results = list(auc = mean(auc),
                 ci95_auc_lo = mean(ci95_auc_lo),
                 ci95_auc_up = mean(ci95_auc_up),
                 precision = mean(precisions),
                 ci95_precision_lo = mean(precisions) - qnorm(0.975)*
                   sd(precisions) / sqrt(nrow(test_matrix)),
                 ci95_precision_up = mean(precisions) + qnorm(0.975)*
                   sd(precisions) / sqrt(nrow(test_matrix)),
                 recall = mean(recalls),
                 ci95_recall_lo = mean(recalls) - qnorm(0.975)*sd(recalls) / 
                   sqrt(nrow(test_matrix)),
                 ci95_recall_up = mean(recalls) + qnorm(0.975)*sd(recalls) /
                   sqrt(nrow(test_matrix)),
                 f_measure = mean(f_measures),
                 ci95_f_measure_lo = mean(f_measures) - qnorm(0.975)*
                   sd(f_measures) / sqrt(nrow(test_matrix)),
                 ci95_f_measure_up = mean(f_measures) + qnorm(0.975)*
                   sd(f_measures) / sqrt(nrow(test_matrix)),
                 lambda = mean(lambdas),
                 ci95_lambdas_lo = mean(lambdas) - qnorm(0.975)*sd(lambdas) /
                   sqrt(nrow(test_matrix)),
                 ci95_lambdas_up = mean(lambdas) + qnorm(0.975)*sd(lambdas) /
                   sqrt(nrow(test_matrix))
                 )
  
  if (alpha == 1){
    results[["nzeros"]] = mean(nzeros)
    results[["ci95_nzeros_lo"]] = mean(nzeros) - qnorm(0.975)*sd(nzeros) /
      sqrt(nrow(test_matrix))
    results[["ci95_nzeros_up"]] = mean(nzeros) + qnorm(0.975)*sd(nzeros) /
      sqrt(nrow(test_matrix))
  }
  
  return(results)
}

lasso_results = lasso_ridge_sim(full_om)
ridge_results = lasso_ridge_sim(full_om, alpha=0)
```

These are the results:

\begin{table}[H]
    \centering
    \begin{tabular}{lllll}
        \toprule
                              & \multicolumn{4}{c}{\textbf{Results}}                                            \\
                              \cmidrule(l r){2-5}                                                               \\
        \textbf{Model}        & AUC       & 95\% CI                    & MSE       & 95\% CI                    \\
        \cdashline{1-5}                                                                                         \\
        LASSO                 & 0.9896788 & [0.9841623, 0.9951954] & 19.46514  & [19.33736, 19.59292]           \\
        Ridge                 & 0.9911772 & [0.9850489, 0.9973056] & 20.33910  & [20.19396, 20.48425]           \\
                              &           &                            &           &                            \\
                              & \multicolumn{4}{c}{\textbf{Specification}}                                      \\
                              \cmidrule(l r){2-5}                                                               \\
        \textbf{Parameter}    & LASSO     & 95\% CI                    & Ridge     & 95\% CI                    \\
        \cdashline{1-5}                                                                                         \\
        $\lambda$             & 0.0404002 & [0.0400770, 0.04072332]    & 0.1147967 & [0.1134965, 0.11609682]    \\
        \# nonzero parameters & 2         & -                          & -         & -                          \\
        \bottomrule
    \end{tabular}
    \caption{Our results}
    \label{tab:ourresults}
\end{table}

Firstly, note that there is no confidence interval available for the number of nonzero parameters for the LASSO model. That is because in all runs, we had the same number of nonzero parameters.

Discuss why do you think the results you obtain are better or worse than Ward (2017); I am looking for an answer that also includes intuition about the specific data you are analyzing.
